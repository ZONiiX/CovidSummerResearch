{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "3d86ca6da76a30d30559e4fb278d6a97482b8c0a5a045b2764928920297b9e26"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "source": [
    "\n",
    "\n",
    "url = \"https://www.sec.gov/cgi-bin/srch-edgar?text=FORM-TYPE=10-k+and+FILING-DATE=202102*\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "rows = soup.findAll('tr')\n",
    "\n",
    "new_rows = rows[12:93]\n",
    "\n",
    "filing_date = []\n",
    "company_name = []\n",
    "form_url = []\n",
    "\n",
    "form = []\n",
    "\n",
    "\n",
    "print(len(rows))\n",
    "print(len(new_rows))\n",
    "\n",
    "for row in new_rows:\n",
    "    data = row.findAll('td')\n",
    "\n",
    "    filingDate = str(data[4]).split(\">\")[1].split('<')[0]\n",
    "\n",
    "    filing_date.append(filingDate)\n",
    "\n",
    "\n",
    "    for company in data[1:2]:\n",
    "            results = company.findAll('a', href=True)\n",
    "            companyName = str(results).split(\">\")[1].split('<')[0]\n",
    "\n",
    "            #print(companyName)\n",
    "            \n",
    "            company_name.append(companyName)\n",
    "\n",
    "    for link in data[2:3]:\n",
    "        new_url = link.findAll('a', href=True)[1]\n",
    "\n",
    "        new_url = \"https://www.sec.gov\" + str(new_url.get('href'))\n",
    "\n",
    "        form_url.append(new_url)\n",
    "\n",
    "#To scrape the second page for the 10-k link, then put the links into another list\n",
    "for link in range(0, len(form_url)):\n",
    "    url = str(form_url[link])\n",
    "\n",
    "    HEADERS = ({'User-Agent':\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "            'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    response = requests.get(url, headers = HEADERS)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    new_url = soup.findAll('a', href=True)[9]\n",
    "\n",
    "    try:\n",
    "        form.append(\"https://www.sec.gov\" + str(new_url.get('href').split('=')[1]))\n",
    "    except:\n",
    "        form.append(\"https://www.sec.gov\" + str(new_url.get('href')))\n",
    "\n",
    "\n",
    "values = {'Company Name': company_name, 'Filing Date': filing_date, '10-k Links': form}\n",
    "\n",
    "df = pd.DataFrame(values).to_csv('formdata.csv', mode='w', index=False)\n",
    "\n",
    "print(len(form))\n",
    "print(len(company_name))\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "94\n",
      "81\n",
      "81\n",
      "81\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}